---
title: "Classification workflow"
format:
  html:
    toc: true
editor: visual
editor_options: 
  chunk_output_type: console
---

```{r include = FALSE}
knitr::knit_hooks$set(
  margin = function(before, options, envir) {
    if (before) par(mgp = c(1.5, .5, 0), bty = "n",
                    plt = c(.105, .97, .13, .97))
    else NULL
  })

knitr::opts_chunk$set(margin     = TRUE,
                      fig.retina = 2,
                      fig.align  = "center")
```

## Limitations and suggestions

Limitations:

-   Small datasets
-   One of the datasets has very low prevalence 6%
-   The fever variable is different in the two datasets
-   We are trying to predict the result of culture when we do know that it depends on other causes that those in our variables (i.e. the quantity of blood used)
-   The durations of symptoms are questionable has it depends on the stage of the disease at which the patient sees the doctor

Suggestions:

-   Switch the role of the 2 datasets in terms of training and testing
-   Proceed in 2 steps: first build the best possible model that will used as a performance reference. Then build a clinical score and measure its performance by comparing with the best possible model

## Global parameters

The path to the data:

```{r}
data_path <- paste0(Sys.getenv("HOME"), "/Library/CloudStorage/",
                    "OneDrive-OxfordUniversityClinicalResearchUnit/",
                    "GitHub/choisy/typhoid/")
```

```{r include = FALSE}
make_path <- function(x) paste0(data_path, "cache/", x)
file_exists <- function(x) file.exists(make_path(x))
readRDS2 <- function(x) readRDS(make_path(x))
saveRDS2 <- function(object, file) saveRDS(object, make_path(file))
```


## Package

Required packages:

```{r}
required_packages <- c("dplyr", "purrr", "rsample", "recipes", "themis", "parsnip",
                       "workflows", "yardstick", "tune", "finetune", "dials",
                       "randomForest")
```

Installing those that are not installed:

```{r}
to_inst <- required_packages[! required_packages %in% installed.packages()[,"Package"]]
if (length(to_inst)) install.packages(to_inst)
```

Loading some for interactive use:

```{r message = FALSE}
library(dplyr)
library(purrr)
library(rsample)
library(recipes)
library(themis)
library(parsnip)
library(workflows)
library(yardstick)
library(tune)
library(finetune)
library(dials)
```


## Utilitary functions

A function that reads a clean data set:

```{r}
read_clean_data <- function(file) readRDS(paste0(data_path, "clean_data/", file))
```

A function that transforms a logical vector into a factor vector (with the correct
number of levels):

```{r}
logical2factor <- function(x) factor(x, c("FALSE", "TRUE"))
```

This function builds an `rsplit` object by combining independent train and test data
sets:

```{r}
make_splits2 <- function(train, test) {
  n_train <- nrow(train)
  make_splits(list(analysis   = seq(n_train),
                   assessment = n_train + seq(nrow(test))),
              bind_rows(train, test))
}
```

Tuning the `collect_metrics()` function:

```{r}
collect_metrics2 <- function(...) {
  collect_metrics(...) |> 
    select(-.config, -.estimator)
}
```

Tuning the `last_fit()` function:

```{r}
last_fit2 <- function(wf, split, metrics) {
# the external validation:
  external <- wf |> 
    last_fit(split, metrics = metrics) |> 
    collect_metrics2() |> 
    select(-.metric)
    
# the internal cross-validation:
  wf |>
    fit_resamples(vfold_cv(training(split)), metrics = metrics) |> 
    collect_metrics2() |> 
    bind_cols(external) # to which we stick the external one.
}
```

## Reading the clean data

The Nepal dataset:

```{r}
nepal <- "nepal.rds" |> 
  read_clean_data() |> 
  mutate(across(c(cough, diarrhea, vomiting, abdominal_pain, constipation, headache),
                as.logical),
         across(c(age, platelets), as.numeric),
         across(fever, ~ .x > 2),
         across(where(is.logical), logical2factor)) |> 
  select(-starts_with("score")) |> 
  select(-fever) |> 
  na.exclude()
```

The Cambodia and Bangladesh dataset:

```{r}
cambodia_bangladesh <- "cambodia_bangladesh.rds" |> 
  read_clean_data() |> 
  mutate(across(fever, ~ .x > 2),
         across(where(is.logical), logical2factor)) |> 
  select(-country) |> 
  select(-fever) |> 
  na.exclude()
```

Checking the consistency of the levels of the factors between the two datasets:

```{r}
get_levels <- function(x) x |>
  select(sex, IgM, CRP) |> 
  map(levels)

identical(get_levels(nepal), get_levels(cambodia_bangladesh))
rm(get_levels)
```

## The metric

We use the ROC AUC to measure the performance of the models:

```{r}
the_metric <- metric_set(roc_auc)
```

## The data splits

Let's consider the two of the two data sets:

```{r}
# train on Cambodia and Bangladesh and test on Nepal:
splits1 <- make_splits2(cambodia_bangladesh, nepal)
# train on Nepal and test on Cambodia and Bangladesh:
splits2 <- make_splits2(nepal, cambodia_bangladesh)
```

## The models

Let's consider 5 types of models:

```{r}
logistic_regression <- logistic_reg("classification", "glm")
logistic_regression_lasso <- logistic_reg("classification", "glmnet",
                                          penalty = tune())
logistic_regression_elasticnet <- logistic_reg("classification", "glmnet",
                                               penalty = tune(), mixture = tune())
random_forest <- rand_forest("classification", "randomForest")
random_forest_tuned <- rand_forest("classification", "randomForest",
                                   mtry = tune(), trees = tune(), min_n = tune())
```

## Functions to tune workflow

The function that builds a workflow without tuning:

```{r}
workflow_no_tune <- function(model, splits) {
  recipe(culture ~ ., training(splits)) |>
    step_unorder(all_ordered_predictors()) |> # adding this
    step_dummy(all_factor_predictors()) |> 
    step_smotenc(culture) |> 
    workflow(model)
}
```

The function that builds a workflow and tune it:

```{r}
workflow_tune <- function(model, split, size = 25, metric = the_metric,
                          mtry_range = c(1, 10)) {
  
  check_mtry <- function(x) {
    if (x$component[1] == "rand_forest") {
      if (any(map_chr(x$object[[1]]$range, class) == "call")) {
        return(update(x, mtry = mtry(mtry_range)))
      }
    }
    x
  }
    
  select_best2 <- function(...)
    select_best(..., metric = names(attributes(metric)$metrics))

  wkflw <- workflow_no_tune(model, split)
  
  grid <- wkflw |> 
    extract_parameter_set_dials() |> 
    check_mtry() |> 
    grid_space_filling(size = size)
  
  resamples <- vfold_cv(training(split))

  grid_search <- tune_grid(wkflw, resamples = resamples, grid = grid, metrics = metric)
    
  iterative_search <- safely(tune_bayes)(wkflw, resamples = resamples,
                                         initial = grid_search, metrics = metric)
  
  output <- list(grid_search = finalize_workflow(wkflw, select_best2(grid_search)))
  if (is.null(iterative_search$result)) return(output)
  output$iterative_search <- finalize_workflow(wkflw,
                                               select_best2(iterative_search$result))
  output
}
```

Wrappers around the above 2 functions to build and possibly tune several models at
once:

```{r}
workflows_tune <- function(models, split, size = 25, metric = the_metric,
                           mtry_range = c(1, 10)) {
  models |> 
    map(workflow_tune, split, size, metric, mtry_range) |> 
    unlist(FALSE) |> 
    unname()
}
```

and:

```{r}
workflows_no_tune <- function(models, splits) map(models, workflow_no_tune, splits)
```


## Models comparisons

Making all the workflows:

```{r}
make_workflows <- function(splits) {
  non_tuned_workflows <- workflows_no_tune(list(logistic_regression, random_forest),
                                           splits)
  
  tuned_workflows <- workflows_tune(list(logistic_regression_elasticnet,
                                         logistic_regression_lasso, random_forest_tuned),
                                    splits)
  
  unlist(list(non_tuned_workflows, tuned_workflows), FALSE)
}
```

Takes 9'42":

```{r eval = FALSE}
workflows <- map(list(splits1, splits2), make_workflows)
```

```{r include = FALSE}
if (file_exists("workflows.rds")) {
  workflows <- readRDS2("workflows.rds")
} else {
  workflows <- map(list(splits1, splits2), make_workflows)
  saveRDS2(workflows, "workflows.rds")
}
```

Takes 67":

```{r eval = FALSE}
workflows_performances <- map2(workflows, list(splits1, splits2),
                               ~ map(.x, last_fit2, .y, the_metric)) |> bind_rows()
```

```{r include = FALSE}
if (file_exists("workflows_performances.rds")) {
  workflows_performances <- readRDS2("workflows_performances.rds")
} else {
  workflows_performances <- map2(workflows, list(splits1, splits2),
                               ~ map(.x, last_fit2, .y, the_metric)) |> bind_rows()
  saveRDS2(workflows_performances, "workflows_performances.rds")
}
```

```{r include = FALSE, eval = FALSE}
if (file_exists("workflows_performances.rds")) {
  workflows_performances <- readRDS2("workflows_performances.rds")
} else {
  workflows_performances <- map2(workflows0,
                                 list(splits1, splits2),
                                 ~ .x |>
                                   unlist(FALSE) |>
                                   map(last_fit2, .y, the_metric)) |> bind_rows()
  saveRDS2(workflows_performances, "workflows_performances.rds")
}
```

```{r}
tibble(
    train  = rep(c("CamBan", "Nepal"), each = 8), 
    model  = rep(c("LR", "RF", rep(c("LS_EN", "LR_lasso", "RF_tuned"), each = 2)), 2),
    search = rep(c(rep(NA, 2), rep(c("grid", "iter"), 3)), 2)) |>
  bind_cols(workflows_performances)
```

```{r}
opar <- par(pty = "s")
with(workflows_performances, {
  min_val <- min(mean, .estimate)
  plot(mean, .estimate, col = rep(c(2, 4), each = 8), cex = 2,
       xlim = c(min_val, 1), ylim = c(min_val, 1),
       xlab = "mean training internal cross-validation", ylab = "testing dataset")
})
abline(0, 1)
box(bty = "o")
par(opar)
```

```{r}
opar <- par(pty = "s")
with(workflows_performances,
     plot(mean, .estimate, col = rep(c(2, 4), each = 8), cex = 2,
          xlim = 0:1, ylim = 0:1,
          xlab = "mean training internal cross-validation", ylab = "testing dataset"))
abline(0, 1)
box(bty = "o")
par(opar)
```



